{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190e1243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\obrie\\anaconda3\\lib\\site-packages (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium) (1.21.5)\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium) (4.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium) (4.11.3)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.8.0)\n",
      "Requirement already satisfied: gymnasium[classic-control] in c:\\users\\obrie\\anaconda3\\lib\\site-packages (0.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (1.21.5)\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (0.2.0)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (0.0.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (4.11.3)\n",
      "Requirement already satisfied: pygame==2.1.3.dev8 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from gymnasium[classic-control]) (2.1.3.dev8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[classic-control]) (3.8.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\obrie\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.1.21)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\obrie\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install gymnasium[classic-control]\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c83d0b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use \"pip install [PACKAGE_NAME]\" to get any required packages you don't have\n",
    "\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f687339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# options: original, plus_velocity, human\n",
    "reward_type = \"original\"\n",
    "episodes = 50\n",
    "\n",
    "# checks that GPU is being used\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10d796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.disable_interactive_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc2bec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_time = datetime.now()\n",
    "time_stamp = curr_time.timestamp()\n",
    "date_time = datetime.fromtimestamp(time_stamp)\n",
    "\n",
    "date = str(date_time)[0:10]\n",
    "time = str(date_time)[11:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001a25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.997\n",
    "        self.learning_rate = 0.002\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=2))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Dense(32))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "                      optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future  = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(Q_future)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99d32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state, next_state, reward_type):\n",
    "    if reward_type == \"original\":\n",
    "        if next_state[0] >= 0.5:\n",
    "            print(\"Car has reached the goal\")\n",
    "            return 100\n",
    "        if next_state[0] > -0.4:\n",
    "            return (1+next_state[0])**2\n",
    "        return 0\n",
    "    \n",
    "    elif reward_type == \"plus_velocity\":\n",
    "        if next_state[0] >= 0.5:\n",
    "            print(\"Car has reached the goal\")\n",
    "            return 100\n",
    "        # if the next action goes higher or has greater speed, reward\n",
    "        if next_state[0] > state[0][0] or abs(next_state[1]) > abs(state[0][1]):\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "    \n",
    "    elif reward_type == \"human\":\n",
    "        if next_state[0] >= 0.5:\n",
    "            print(\"Car has reached the goal\")\n",
    "            return 100\n",
    "        # if slowing down and going higher, reward\n",
    "        if next_state[0] > state[0][0] and abs(next_state[1]) < abs(state[0][1]):\n",
    "            return 1\n",
    "        # if speeding up and going lower, reward\n",
    "        if next_state[0] < state[0][0] and abs(next_state[1]) > abs(state[0][1]):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8f50e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:  2\n",
      "action size:  3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode = None)\n",
    "\n",
    "np.random.seed(458)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "print('state size: ', state_size)\n",
    "print('action size: ', action_size)\n",
    "done = False\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb297418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0\n",
      "time:  0\n",
      "time:  10\n",
      "time:  20\n",
      "time:  30\n",
      "time:  40\n",
      "time:  50\n",
      "time:  60\n",
      "time:  70\n",
      "time:  80\n",
      "time:  90\n",
      "time:  100\n",
      "time:  110\n",
      "time:  120\n",
      "time:  130\n",
      "time:  140\n",
      "time:  150\n",
      "time:  160\n",
      "time:  170\n",
      "time:  180\n",
      "time:  190\n",
      "episode: 0/50, score: 199, e: 0.66\n",
      "saving the model\n",
      "episode:  1\n",
      "time:  0\n",
      "time:  10\n",
      "time:  20\n",
      "time:  30\n",
      "time:  40\n",
      "time:  50\n",
      "time:  60\n",
      "time:  70\n",
      "time:  80\n",
      "time:  90\n",
      "time:  100\n",
      "time:  110\n",
      "time:  120\n",
      "time:  130\n",
      "time:  140\n",
      "time:  150\n",
      "time:  160\n",
      "time:  170\n",
      "time:  180\n",
      "time:  190\n",
      "episode: 1/50, score: 199, e: 0.36\n",
      "episode:  2\n",
      "time:  0\n",
      "time:  10\n",
      "time:  20\n",
      "time:  30\n",
      "time:  40\n",
      "time:  50\n",
      "time:  60\n",
      "time:  70\n",
      "time:  80\n",
      "time:  90\n",
      "time:  100\n",
      "time:  110\n",
      "time:  120\n",
      "time:  130\n",
      "time:  140\n",
      "time:  150\n",
      "time:  160\n",
      "time:  170\n",
      "time:  180\n",
      "time:  190\n",
      "episode: 2/50, score: 199, e: 0.2\n",
      "episode:  3\n",
      "time:  0\n",
      "time:  10\n",
      "time:  20\n",
      "time:  30\n",
      "time:  40\n",
      "time:  50\n",
      "time:  60\n",
      "time:  70\n",
      "time:  80\n",
      "time:  90\n",
      "time:  100\n",
      "time:  110\n",
      "time:  120\n",
      "time:  130\n",
      "time:  140\n",
      "time:  150\n",
      "episode: 3/50, score: -2359.0, e: 0.12\n",
      "episode:  4\n",
      "time:  0\n",
      "time:  10\n",
      "time:  20\n",
      "time:  30\n",
      "time:  40\n",
      "time:  50\n",
      "time:  60\n",
      "time:  70\n",
      "time:  80\n",
      "time:  90\n",
      "time:  100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20472\\1627263629.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mflag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20472\\2874354533.py\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mminibatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = 0\n",
    "step_history = []\n",
    "\n",
    "for e in range(episodes):\n",
    "    print(\"episode: \", e)\n",
    "    state = env.reset()[0]\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    flag = 0\n",
    "    for time in range(200):\n",
    "        if time % 10 == 0:\n",
    "            print(\"time: \", time)\n",
    "        # uncomment this to see the actual rendering \n",
    "        #env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)[0:4]\n",
    "\n",
    "        if next_state[1] > state[0][1] and next_state[1]>0 and state[0][1]>0:\n",
    "            reward += 15\n",
    "        elif next_state[1] < state[0][1] and next_state[1]<=0 and state[0][1]<=0:\n",
    "            reward +=15\n",
    "        \n",
    "#         reward += get_reward(state, next_state, reward_type)\n",
    "\n",
    "\n",
    "        # give more reward if the cart reaches the flag in 200 steps\n",
    "        if done:\n",
    "            reward += 100\n",
    "        else:\n",
    "            # put a penalty if the no of time steps is more\n",
    "            reward -= 10  \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        scores += reward\n",
    "        if done:\n",
    "            flag = 1\n",
    "            agent.update_target_model()\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, episodes, scores, agent.epsilon))\n",
    "            step_history.append(time)\n",
    "            break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "            \n",
    "    if flag == 0:\n",
    "        print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, episodes, time, agent.epsilon))   \n",
    "        step_history.append(time)\n",
    "    if e % (episodes/5) == 0:\n",
    "        print('saving the model')\n",
    "        agent.save(\"./MC_models_{}_{}_{}/mountain_car-dqn_{}.h5\".format(reward_type, date, time, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e62197",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\"original\" : \"blue\", \"plus_velocity\" : \"green\", \"human\" : \"red\"}\n",
    "\n",
    "plt.plot(step_history, color = colors[reward_type])\n",
    "plt.ylabel('Steps per Episode')\n",
    "plt.title(\"Mountain Car Training Steps with {} Reward Function\".format(reward_type))\n",
    "plt.show()\n",
    "plt.savefig(\"./MC_models_{}_{}_{}/mountain_car-dqn_image.png\".format(reward_type, date, time, e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
