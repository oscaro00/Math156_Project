{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3282166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gymnasium\n",
    "pip install gymnasium[classic-control]\n",
    "pip install imageio==2.4.1\n",
    "apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "pip install colabgymrender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae2166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use \"pip install [PACKAGE_NAME]\" to get any required packages you don't have\n",
    "\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from colabgymrender.recorder import Recorder # this line is only necessary on google colab\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c87db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options: original, plus_velocity, human\n",
    "reward_type = \"original\"\n",
    "episodes = 50\n",
    "\n",
    "# checks that GPU is being used\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_time = datetime.now()\n",
    "time_stamp = curr_time.timestamp()\n",
    "date_time = datetime.fromtimestamp(time_stamp)\n",
    "\n",
    "date = str(date_time)[0:10]\n",
    "time = str(date_time)[11:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f667cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.997\n",
    "        self.learning_rate = 0.002\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=2))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Dense(32))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future  = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(Q_future)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state, next_state, reward_type):\n",
    "    if reward_type == \"original\":\n",
    "        if next_state[0] >= 0.5:\n",
    "            print(\"Car has reached the goal\")\n",
    "            return 10\n",
    "        if next_state[0] > -0.4:\n",
    "            return (1+next_state[0])**2\n",
    "        return 0\n",
    "    \n",
    "    elif reward_type == \"plus_velocity\":\n",
    "        if next_state[0] >= 0.5:\n",
    "            print(\"Car has reached the goal\")\n",
    "            return 10\n",
    "        # if the next action goes higher or has greater speed, reward\n",
    "        if next_state[0] > state[0][0] or abs(next_state[1]) > abs(state[0][1]):\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "    \n",
    "    elif reward_type == \"human\":\n",
    "        if next_state[0] >= 0.5:\n",
    "            print(\"Car has reached the goal\")\n",
    "            return 10\n",
    "        # if slowing down and going higher, reward\n",
    "        if next_state[0] > state[0][0] and abs(next_state[1]) < abs(state[0][1]):\n",
    "            return 1\n",
    "        # if speeding up and going lower, reward\n",
    "        if next_state[0] < state[0][0] and abs(next_state[1]) > abs(state[0][1]):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20697e3",
   "metadata": {},
   "source": [
    "# if the Recorder breaks above, remove \"mode = 'rgb_array'\" on lines 20,46\n",
    "\n",
    "# must restart runtime after editing lines, but don't re-run pip lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode = \"rgb_array\")\n",
    "directory = './MC_videos_{}_{}_{}'.format(reward_type, date, time)\n",
    "env = Recorder(env, directory)\n",
    "\n",
    "np.random.seed(458)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "print('state size:' ,state_size)\n",
    "print('action size: ', action_size)\n",
    "done = False\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = 0\n",
    "step_history = []\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    flag = 0\n",
    "    for time in range(200):\n",
    "        # uncomment this to see the actual rendering \n",
    "        #env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "#         if next_state[1] > state[0][1] and next_state[1]>0 and state[0][1]>0:\n",
    "#             reward += 15\n",
    "#         elif next_state[1] < state[0][1] and next_state[1]<=0 and state[0][1]<=0:\n",
    "#             reward +=15\n",
    "        \n",
    "        reward += get_reward(state, next_state, reward_type)\n",
    "\n",
    "\n",
    "        # give more reward if the cart reaches the flag in 200 steps\n",
    "#         if done:\n",
    "#             reward += 100\n",
    "#         else:\n",
    "#             # put a penalty if the no of time steps is more\n",
    "#             reward -= 10  \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        scores += reward\n",
    "        if done:\n",
    "            flag = 1\n",
    "            agent.update_target_model()\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, scores, agent.epsilon))\n",
    "            step_history.append(time)\n",
    "            break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "            \n",
    "    if flag == 0:\n",
    "        print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, episodes, time, agent.epsilon)) \n",
    "        step_history.append(time)\n",
    "    if e % (episodes/5) == 0:\n",
    "        print('saving the model')\n",
    "        agent.save(\"./MC_models_{}_{}_{}/mountain_car-dqn_{}.h5\".format(reward_type, date, time, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\"original\" : \"blue\", \"plus_velocity\" : \"green\", \"human\" : \"red\"}\n",
    "\n",
    "plt.plot(step_history, color = colors[reward_type])\n",
    "plt.ylabel('Steps per Episode')\n",
    "plt.title(\"Mountain Car Training Steps with {} Reward Function\".format(reward_type))\n",
    "plt.show()\n",
    "plt.savefig(\"./MC_models_{}_{}_{}/mountain_car-dqn_image.png\".format(reward_type, date, time, e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
